# -*- coding: utf-8 -*-
"""Electrical-Fault-Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NM64GFS6EfTTnNFneuFYGcr9pUESdGM9

# Load Dataset
"""

import pandas as pd
Data = pd.read_csv('/content/detect_dataset.csv')
Data.head()

Data.shape

Data.isnull().sum()

Data.drop(columns = ['Unnamed: 7', 'Unnamed: 8'], axis=1, inplace = True)

Data.shape

Data.head()

Data.duplicated().sum()

"""# EDA"""

Data['Output (S)'].unique()

Data['Output (S)'].value_counts()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Sample DataFrame (replace this with your actual DataFrame)
# Data = pd.read_csv('your_file.csv')

# Get the unique values and their frequencies
unique_values = Data['Output (S)'].unique()
frequency_counts = Data['Output (S)'].value_counts()

# Plotting
plt.figure(figsize=(3, 3))
sns.barplot(x=frequency_counts.index, y=frequency_counts.values, palette='viridis')

# Adding labels and title
plt.xlabel('Unique Values')
plt.ylabel('Frequency')
plt.title('Frequency of Unique Values in Output (S)')
plt.show()

plt.bar(x = Data.columns , height = Data.corr()['Output (S)'])
plt.axhline(y=0, color='black', linewidth=1)
plt.title('Correlation between Ouput(S) and other columns')
plt.show()

"""la , lb and lc peak around 0 only
Va , Vb and Vc peak around 0 and also have more spreaded distiribution
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'Data' is your DataFrame containing columns 'Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc'

# Selecting columns of interest
columns_to_plot = ['Ia', 'Ib', 'Ic', 'Va', 'Vb', 'Vc']

# Creating separate box plots for each column
plt.figure(figsize=(5, 5))

for i, column in enumerate(columns_to_plot, start=1):
    plt.subplot(2, 3, i)
    sns.boxplot(x=Data[column], palette='Set2')
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.ylabel('Values')

plt.tight_layout()
plt.show()

"""la , lb, and lc have a very little spread and quite a lot of outliers
Va , Vb and Vc have more spread and no outliers
"""

plt.figure(figsize=(10, 6))
sns.heatmap(Data.drop('Output (S)',axis=1).corr(), annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.show()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(Data.drop('Output (S)',axis=1),Data['Output (S)'],random_state=42,test_size=0.2)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.metrics import accuracy_score

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
LG = LogisticRegression()
LG.fit(x_train,y_train)

X_Train_Pre = LG.predict(x_train)

LG_X_trainAccuracy = accuracy_score(X_Train_Pre , y_train)
LG_X_trainAccuracy

X_Test_Pred = LG.predict(x_test)

LG_X_testAaccuracy = accuracy_score(X_Test_Pred, y_test)
LG_X_testAaccuracy

"""# Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier()
DT.fit(x_train, y_train)

X_Train_Pre = DT.predict(x_train)

DT_X_trainAccuracy = accuracy_score(X_Train_Pre , y_train)
DT_X_trainAccuracy

X_Test_Pred = DT.predict(x_test)

DT_X_testAaccuracy = accuracy_score(X_Test_Pred, y_test)
DT_X_testAaccuracy

"""# Support Vector Machine"""

from sklearn.svm import SVC
SV = SVC()
SV.fit(x_train, y_train)

X_train = SV.predict(x_train)

SV_X_trainAccuracy = accuracy_score(X_train , y_train)
SV_X_trainAccuracy

X_test = SV.predict(x_test)

SV_X_testAccuracy = accuracy_score(X_test , y_test)
SV_X_testAccuracy

"""# KNeighbours Classifier"""

from sklearn.neighbors import KNeighborsClassifier
KNC = KNeighborsClassifier()
KNC.fit(x_train, y_train)

X_train = KNC.predict(x_train)

KNC_X_trainAccuracy = accuracy_score(X_train , y_train)
KNC_X_trainAccuracy

X_test = KNC.predict(x_test)

KNC_X_testAccuracy = accuracy_score(X_test, y_test)
KNC_X_testAccuracy

models = pd.DataFrame({
    'Model' : ['Logistic Regression', 'Decision Tree Classifier'
             ,'Support Vector Machine', 'KNeighbors Classifier'],
    'Score' : [LG_X_testAaccuracy ,DT_X_testAaccuracy , SV_X_testAccuracy , KNC_X_testAccuracy]
})


models.sort_values(by = 'Score', ascending = False)

import plotly.express as px

# Assuming 'models' is your DataFrame with model scores
models = pd.DataFrame({
    'Model': ['Logistic Regression', 'Decision Tree Classifier' , 'Support Vector Machine' , 'KNeighbors Classifier'],
    'Score': [73,99,98,99]
})

fig = px.bar(data_frame=models, x='Score', y='Model', color='Score', template='plotly_dark',
             title='Models Comparison')

# Set the size of the graph
fig.update_layout(height=400, width=600)

fig.show()

"""# ANN"""

import tensorflow
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense,Dropout

model = Sequential([
    Dense(256,'relu',input_dim=x_train.shape[1]),
    Dropout(0.5),
    Dense(128,'relu'),
    Dropout(0.5),
    Dense(1,'sigmoid'),
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
history = model.fit(x_train,y_train,validation_data = (x_test,y_test),epochs=10)

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,3))
ax1.plot(history.history['loss'],label='training')
ax1.plot(history.history['val_loss'],label='validation')
ax1.set_title('loss')
ax1.set_xlabel('epoch')
ax1.set_ylabel('loss')

ax2.plot(history.history['accuracy'])
ax2.plot(history.history['val_accuracy'])
ax2.set_title('accuracy')
ax2.set_xlabel('epoch')
ax2.set_ylabel('accuracy')

plt.subplots_adjust(wspace=0.3)
fig.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), fancybox=True, shadow=True, ncol=2)
plt.show()